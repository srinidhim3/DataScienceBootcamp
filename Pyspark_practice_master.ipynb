{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Let's practice Pyspark by cooking up some code"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from pyspark.sql import SparkSession"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "%config Completer.use_jedi = False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "spark = SparkSession.builder.master('local[1]').appName('SparkByExample.com').getOrCreate()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "rdd = spark.sparkContext.parallelize([1,2,3,4,5])\n",
    "rdd"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "type(rdd)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Note:** RDD are fault tolerant and immutable distributed collections of objects and divided into logical partitions computed on different nodes of cluster."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RDD\n",
    "* Created using `parallelize()`\n",
    "* text file `textfile()`\n",
    "* Operations\n",
    "    * Transformations : lazy operations, instead of updating a current RDD, these operations returns another RDD. Examples `flatmap(), map(), reduceByKey(), filter(), SortByKey()`\n",
    "    * Actions : operations that trigger computation and return RDD values to the driver. Examples `count(), collect(), first(), max(), reduce()`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# RDD's can be created using parallelize method from list\n",
    "rddcollect = rdd.collect()\n",
    "rddcollect"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "print(f'Number of partitions: {str(rdd.getNumPartitions())}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of partitions: 1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "print(f'Action : First Element: {str(rdd.first())}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Action : First Element: 1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "print(f'is Empty RDD: {str(rdd.isEmpty())}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "is Empty RDD: False\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RDD Repartition vs Collasce\n",
    "repartition() is used to increase or decrease the RDD/DataFrame partitions whereas the PySpark coalesce() is used to only decrease the number of partitions in an efficient way. \n",
    "\n",
    "üëáÔ∏è These are very expensive operations as they shuffle the data across many partitions hence try to minimize using these as much as possible.üëáÔ∏è"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "rdd = spark.sparkContext.parallelize((0, 20))\n",
    "print(f'default partitions : {str(rdd.getNumPartitions())}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "default partitions : 1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "rdd1 = spark.sparkContext.parallelize(range(0,36), 6)\n",
    "print(f'configured partitions : {str(rdd.getNumPartitions())}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "configured partitions : 1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "rddfromfile = spark.sparkContext.textFile('textfile.txt', 10)\n",
    "rddfromfile"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "textfile.txt MapPartitionsRDD[6] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "print(f'textfile partitions : {str(rddfromfile.getNumPartitions())}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "textfile partitions : 11\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "! rm -rf partition\n",
    "rdd1.saveAsTextFile('partition')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "rdd2 = rdd1.repartition(4)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "print(f'Repartitioned size : {str(rdd2.getNumPartitions())}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Repartitioned size : 4\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "! rm -rf repartitioned\n",
    "rdd2.saveAsTextFile('repartitioned')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "rdd3 = rdd1.coalesce(4)\n",
    "print(f'coalesce partitions : {str(rdd3.getNumPartitions())}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "coalesce partitions : 4\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "!rm -rf coalesce_files\n",
    "rdd3.saveAsTextFile('coalesce_files')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "rdd4 = rdd1.coalesce(10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DataFrame repartition() vs coalesce()\n",
    "\n",
    "üëÜÔ∏è you can‚Äôt specify the partition/parallelism while creating DataFrame. DataFrame by default internally uses the methods specified in Section 1 to determine the default partition and splits the data for parallelism.üëÜÔ∏è"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "df = spark.range(0,20)\n",
    "df"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[id: bigint]"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "df.collect()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Row(id=0),\n",
       " Row(id=1),\n",
       " Row(id=2),\n",
       " Row(id=3),\n",
       " Row(id=4),\n",
       " Row(id=5),\n",
       " Row(id=6),\n",
       " Row(id=7),\n",
       " Row(id=8),\n",
       " Row(id=9),\n",
       " Row(id=10),\n",
       " Row(id=11),\n",
       " Row(id=12),\n",
       " Row(id=13),\n",
       " Row(id=14),\n",
       " Row(id=15),\n",
       " Row(id=16),\n",
       " Row(id=17),\n",
       " Row(id=18),\n",
       " Row(id=19)]"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "print(type(df))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "print(f'Dataframe partitions : {str(df.rdd.getNumPartitions())}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataframe partitions : 1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "df.write.mode('overwrite').csv('dataframe_partitions.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "df2 = df.repartition(6)\n",
    "print(f'repartitioned dataframe partitions : {str(df2.rdd.getNumPartitions())}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "repartitioned dataframe partitions : 6\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "df.write.mode('overwrite').csv('df2')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "df3 = df.coalesce(2)\n",
    "print(f'repartitioned dataframe partitions : {str(df3.rdd.getNumPartitions())}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "repartitioned dataframe partitions : 1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "df4 = df.groupBy('id').count()\n",
    "print(df4.rdd.getNumPartitions())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('spark.app.id', 'local-1632466828817'),\n",
       " ('spark.app.startTime', '1632466827726'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.app.name', 'SparkByExample.com'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.driver.port', '36707'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.master', 'local[1]'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  'file:/home/srinidhi/Documents/pyspark%20practice/spark-warehouse'),\n",
       " ('spark.driver.host', 'fedora')]"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RDD Broadcast variables\n",
    "* PySpark breaks the job into stages that have distributed shuffling and actions are executed with in the stage.\n",
    "* Later Stages are also broken into tasks\n",
    "* Spark broadcasts the common data (reusable) needed by tasks within each stage.\n",
    "* The broadcasted data is cache in serialized format and deserialized before executing each task."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "states = {'NY': 'New York', 'CA': 'California'}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "broadcastStates = spark.sparkContext.broadcast(states)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "rdd = spark.sparkContext.parallelize(data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "def state_convert(code):\n",
    "    return broadcastStates.value[code]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "result = rdd.map(lambda x : (x[0], x[1], x[2], state_convert(x[3])))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "def state_convert(code):\n",
    "    return broadcastStates.value[code]\n",
    "\n",
    "result = rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).collect()\n",
    "print(result)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('James', 'Smith', 'USA', 'California'), ('Michael', 'Rose', 'USA', 'New York'), ('Robert', 'Williams', 'USA', 'California'), ('Maria', 'Jones', 'USA', 'Florida')]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DataFrame Broadcast variables"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "def state_convert(code):\n",
    "    return broadcastStates.value[code]\n",
    "\n",
    "result = df.rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).toDF(columns)\n",
    "result.show(truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n",
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|James    |Smith   |USA    |CA   |\n",
      "|Michael  |Rose    |USA    |NY   |\n",
      "|Robert   |Williams|USA    |CA   |\n",
      "|Maria    |Jones   |USA    |FL   |\n",
      "+---------+--------+-------+-----+\n",
      "\n",
      "+---------+--------+-------+----------+\n",
      "|firstname|lastname|country|state     |\n",
      "+---------+--------+-------+----------+\n",
      "|James    |Smith   |USA    |California|\n",
      "|Michael  |Rose    |USA    |New York  |\n",
      "|Robert   |Williams|USA    |California|\n",
      "|Maria    |Jones   |USA    |Florida   |\n",
      "+---------+--------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RDD Accumulator\n",
    "* perform sum and counter operations similar to Map-reduce counters. These variables are shared by all executors to update and add information through aggregation or computative operations.\n",
    "* Accumulators are write-only and initialize once variables where only tasks that are running on workers are allowed to update and updates from the workers get propagated automatically to the driver program. But, only the driver program is allowed to access the Accumulator variable using the value property.\n",
    "* Using `accumulator()` from SparkContext class we can create an Accumulator in PySpark programming. Users can also create Accumulators for custom types using AccumulatorParam class of PySpark.\n",
    "* `add()` function is used to add/update a value in accumulator\n",
    "* `value` property on the accumulator variable is used to retrieve the value from the accumulator."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "accum = spark.sparkContext.accumulator(0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "rdd = spark.sparkContext.parallelize([1,2,3,4,5])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "rdd.foreach(lambda x: accum.add(x))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "print(accum.value)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "15\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Note:** `rdd.foreach()` is executed on workers and `accum.value` is called from PySpark driver program."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "accuSum=spark.sparkContext.accumulator(0)\n",
    "def countFun(x):\n",
    "    global accuSum\n",
    "    accuSum+=x\n",
    "rdd.foreach(countFun)\n",
    "print(accuSum.value)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "15\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "accumCount=spark.sparkContext.accumulator(0)\n",
    "rdd2=spark.sparkContext.parallelize([1,2,3,4,5])\n",
    "rdd2.foreach(lambda x:accumCount.add(1))\n",
    "print(accumCount.value)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Aggregate functions \n",
    "* PySpark provides built-in standard Aggregate functions defines in DataFrame API, these come in handy when we need to make aggregate operations on DataFrame columns. Aggregate functions operate on a group of rows and calculate a single return value for every group."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "simpleData = [(\"James\", \"Sales\", 3000),\n",
    "    (\"Michael\", \"Sales\", 4600),\n",
    "    (\"Robert\", \"Sales\", 4100),\n",
    "    (\"Maria\", \"Finance\", 3000),\n",
    "    (\"James\", \"Sales\", 3000),\n",
    "    (\"Scott\", \"Finance\", 3300),\n",
    "    (\"Jen\", \"Finance\", 3900),\n",
    "    (\"Jeff\", \"Marketing\", 3000),\n",
    "    (\"Kumar\", \"Marketing\", 2000),\n",
    "    (\"Saif\", \"Sales\", 4100)\n",
    "  ]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "schema = [\"employee_name\", \"department\", \"salary\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|James        |Sales     |3000  |\n",
      "|Michael      |Sales     |4600  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Maria        |Finance   |3000  |\n",
      "|James        |Sales     |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Saif         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "from pyspark.sql.functions import *\n",
    "df.select(approx_count_distinct('salary')).collect()[0][0]\n",
    "#returns the count of distinct items in a group."
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "df.select(avg('salary')).collect()[0][0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3400.0"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "#collect_list() function returns all values from an input column with duplicates.\n",
    "df.select(collect_list('salary')).show(truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------------------------------------------------------------+\n",
      "|collect_list(salary)                                        |\n",
      "+------------------------------------------------------------+\n",
      "|[3000, 4600, 4100, 3000, 3000, 3300, 3900, 3000, 2000, 4100]|\n",
      "+------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "# collect_set() function returns all values from an input column with duplicate values eliminated.\n",
    "df.select(collect_set('salary')).show(truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------------------------------------+\n",
      "|collect_set(salary)                 |\n",
      "+------------------------------------+\n",
      "|[4600, 3000, 3900, 4100, 3300, 2000]|\n",
      "+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "# countDistinct() function returns the number of distinct elements in a columns\n",
    "df.select(countDistinct('salary')).show(truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------------------+\n",
      "|count(DISTINCT salary)|\n",
      "+----------------------+\n",
      "|6                     |\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Grouping\n",
    "* grouping() Indicates whether a given input column is aggregated or not. returns 1 for aggregated or 0 for not aggregated in the result. If you try grouping directly on the salary column you will get below error"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "# first() function returns the first element in a column when ignoreNulls is set to true, it returns the first non-null element.\n",
    "df.select(first('salary')).show(truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------+\n",
      "|first(salary)|\n",
      "+-------------+\n",
      "|3000         |\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "# last() function returns the last element in a column. when ignoreNulls is set to true, it returns the last non-null element.\n",
    "df.select(last('salary')).show(truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------------+\n",
      "|last(salary)|\n",
      "+------------+\n",
      "|4100        |\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "# kurtosis() function returns the kurtosis of the values in a group.\n",
    "df.select(kurtosis('salary')).show(truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------------+\n",
      "|kurtosis(salary)   |\n",
      "+-------------------+\n",
      "|-0.6467803030303032|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "# max() function returns the maximum value in a column.\n",
    "df.select(max('salary')).show(truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------+\n",
      "|max(salary)|\n",
      "+-----------+\n",
      "|4600       |\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "# mean() function returns the average of the values in a column. Alias for Avg\n",
    "df.select(mean('salary')).show(truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------+\n",
      "|avg(salary)|\n",
      "+-----------+\n",
      "|3400.0     |\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "# skewness() function returns the skewness of the values in a group.\n",
    "df.select(skewness('salary')).show(truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+\n",
      "|skewness(salary)    |\n",
      "+--------------------+\n",
      "|-0.12041791181069571|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Note:**\n",
    "stddev() alias for stddev_samp.\n",
    "\n",
    "stddev_samp() function returns the sample standard deviation of values in a column.\n",
    "\n",
    "stddev_pop() function returns the population standard deviation of the values in a column."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "df.select(stddev(\"salary\"), stddev_samp(\"salary\"), \\\n",
    "    stddev_pop(\"salary\")).show(truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------------+-------------------+------------------+\n",
      "|stddev_samp(salary)|stddev_samp(salary)|stddev_pop(salary)|\n",
      "+-------------------+-------------------+------------------+\n",
      "|765.9416862050705  |765.9416862050705  |726.636084983398  |\n",
      "+-------------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "# sumDistinct() function returns the sum of all distinct values in a column.\n",
    "\n",
    "df.select(sumDistinct(\"salary\")).show(truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+\n",
      "|sum(DISTINCT salary)|\n",
      "+--------------------+\n",
      "|20900               |\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Note:**\n",
    "variance(), var_samp(), var_pop()\n",
    "variance() alias for var_samp\n",
    "\n",
    "var_samp() function returns the unbiased variance of the values in a column.\n",
    "\n",
    "var_pop() function returns the population variance of the values in a column."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "df.select(variance(\"salary\"),var_samp(\"salary\"),var_pop(\"salary\")) \\\n",
    "  .show(truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------------+-----------------+---------------+\n",
      "|var_samp(salary) |var_samp(salary) |var_pop(salary)|\n",
      "+-----------------+-----------------+---------------+\n",
      "|586666.6666666666|586666.6666666666|528000.0       |\n",
      "+-----------------+-----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Window Functions üî•üî•\n",
    "* PySpark Window functions are used to calculate results such as the rank, row number e.t.c over a range of input rows.\n",
    "* To perform an operation on a group first, we need to partition the data using Window.partitionBy() , and for row number and rank function we need to additionally order by on partition data using orderBy clause."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### row_number Window Function üëÜÔ∏èüëç\n",
    "* row_number() window function is used to give the sequential row number starting from 1 to the result of each window partition."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "from pyspark.sql.window import Window"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "windowSpec = Window.partitionBy('department').orderBy('salary')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "df.withColumn('row_number', row_number().over(windowSpec)).show(truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|row_number|\n",
      "+-------------+----------+------+----------+\n",
      "|James        |Sales     |3000  |1         |\n",
      "|James        |Sales     |3000  |2         |\n",
      "|Robert       |Sales     |4100  |3         |\n",
      "|Saif         |Sales     |4100  |4         |\n",
      "|Michael      |Sales     |4600  |5         |\n",
      "|Maria        |Finance   |3000  |1         |\n",
      "|Scott        |Finance   |3300  |2         |\n",
      "|Jen          |Finance   |3900  |3         |\n",
      "|Kumar        |Marketing |2000  |1         |\n",
      "|Jeff         |Marketing |3000  |2         |\n",
      "+-------------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### rank Window Function üëÜÔ∏è\n",
    "* rank() window function is used to provide a rank to the result within a window partition. This function leaves gaps in rank when there are ties."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "df.withColumn('rank', rank().over(windowSpec)).show(truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|rank|\n",
      "+-------------+----------+------+----+\n",
      "|James        |Sales     |3000  |1   |\n",
      "|James        |Sales     |3000  |1   |\n",
      "|Robert       |Sales     |4100  |3   |\n",
      "|Saif         |Sales     |4100  |3   |\n",
      "|Michael      |Sales     |4600  |5   |\n",
      "|Maria        |Finance   |3000  |1   |\n",
      "|Scott        |Finance   |3300  |2   |\n",
      "|Jen          |Finance   |3900  |3   |\n",
      "|Kumar        |Marketing |2000  |1   |\n",
      "|Jeff         |Marketing |3000  |2   |\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### dense_rank Window Function üéØ\n",
    "* `dense_rank()` window function is used to get the result with rank of rows within a window partition without any gaps. This is similar to rank() function difference being rank function leaves gaps in rank when there are ties."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "df.withColumn('Dense_Rank',dense_rank().over(windowSpec)).show(truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|Dense_Rank|\n",
      "+-------------+----------+------+----------+\n",
      "|James        |Sales     |3000  |1         |\n",
      "|James        |Sales     |3000  |1         |\n",
      "|Robert       |Sales     |4100  |2         |\n",
      "|Saif         |Sales     |4100  |2         |\n",
      "|Michael      |Sales     |4600  |3         |\n",
      "|Maria        |Finance   |3000  |1         |\n",
      "|Scott        |Finance   |3300  |2         |\n",
      "|Jen          |Finance   |3900  |3         |\n",
      "|Kumar        |Marketing |2000  |1         |\n",
      "|Jeff         |Marketing |3000  |2         |\n",
      "+-------------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### percent_rank Window Function üòé"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "df.withColumn('Percent_Rank', percent_rank().over(windowSpec)).show(truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------+----------+------+------------+\n",
      "|employee_name|department|salary|Percent_Rank|\n",
      "+-------------+----------+------+------------+\n",
      "|James        |Sales     |3000  |0.0         |\n",
      "|James        |Sales     |3000  |0.0         |\n",
      "|Robert       |Sales     |4100  |0.5         |\n",
      "|Saif         |Sales     |4100  |0.5         |\n",
      "|Michael      |Sales     |4600  |1.0         |\n",
      "|Maria        |Finance   |3000  |0.0         |\n",
      "|Scott        |Finance   |3300  |0.5         |\n",
      "|Jen          |Finance   |3900  |1.0         |\n",
      "|Kumar        |Marketing |2000  |0.0         |\n",
      "|Jeff         |Marketing |3000  |1.0         |\n",
      "+-------------+----------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ntile Window Function üòÉ\n",
    "* `ntile()` window function returns the relative rank of result rows within a window partition. In below example we have used 2 as an argument to ntile hence it returns ranking between 2 values (1 and 2)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "df.withColumn('ntile', ntile(2).over(windowSpec)).show(truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------+----------+------+-----+\n",
      "|employee_name|department|salary|ntile|\n",
      "+-------------+----------+------+-----+\n",
      "|James        |Sales     |3000  |1    |\n",
      "|James        |Sales     |3000  |1    |\n",
      "|Robert       |Sales     |4100  |1    |\n",
      "|Saif         |Sales     |4100  |2    |\n",
      "|Michael      |Sales     |4600  |2    |\n",
      "|Maria        |Finance   |3000  |1    |\n",
      "|Scott        |Finance   |3300  |1    |\n",
      "|Jen          |Finance   |3900  |2    |\n",
      "|Kumar        |Marketing |2000  |1    |\n",
      "|Jeff         |Marketing |3000  |2    |\n",
      "+-------------+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "df.withColumn('ntile', ntile(3).over(windowSpec)).show(truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------+----------+------+-----+\n",
      "|employee_name|department|salary|ntile|\n",
      "+-------------+----------+------+-----+\n",
      "|James        |Sales     |3000  |1    |\n",
      "|James        |Sales     |3000  |1    |\n",
      "|Robert       |Sales     |4100  |2    |\n",
      "|Saif         |Sales     |4100  |2    |\n",
      "|Michael      |Sales     |4600  |3    |\n",
      "|Maria        |Finance   |3000  |1    |\n",
      "|Scott        |Finance   |3300  |2    |\n",
      "|Jen          |Finance   |3900  |3    |\n",
      "|Kumar        |Marketing |2000  |1    |\n",
      "|Jeff         |Marketing |3000  |2    |\n",
      "+-------------+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Window Analytic functions üî•üî•"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### cume_dist Window Function üòá\n",
    "* `cume_dist()` window function is used to get the cumulative distribution of values within a window partition.\n",
    "\n",
    "* This is the same as the DENSE_RANK function in SQL."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "df.withColumn('cume_dist', cume_dist().over(windowSpec)).show(truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------+----------+------+------------------+\n",
      "|employee_name|department|salary|cume_dist         |\n",
      "+-------------+----------+------+------------------+\n",
      "|James        |Sales     |3000  |0.4               |\n",
      "|James        |Sales     |3000  |0.4               |\n",
      "|Robert       |Sales     |4100  |0.8               |\n",
      "|Saif         |Sales     |4100  |0.8               |\n",
      "|Michael      |Sales     |4600  |1.0               |\n",
      "|Maria        |Finance   |3000  |0.3333333333333333|\n",
      "|Scott        |Finance   |3300  |0.6666666666666666|\n",
      "|Jen          |Finance   |3900  |1.0               |\n",
      "|Kumar        |Marketing |2000  |0.5               |\n",
      "|Jeff         |Marketing |3000  |1.0               |\n",
      "+-------------+----------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### lag Window Function üö©\n",
    "* returns the value from previous row"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "df.withColumn('lag', lag('salary',2).over(windowSpec)).show(truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|lag |\n",
      "+-------------+----------+------+----+\n",
      "|James        |Sales     |3000  |null|\n",
      "|James        |Sales     |3000  |null|\n",
      "|Robert       |Sales     |4100  |3000|\n",
      "|Saif         |Sales     |4100  |3000|\n",
      "|Michael      |Sales     |4600  |4100|\n",
      "|Maria        |Finance   |3000  |null|\n",
      "|Scott        |Finance   |3300  |null|\n",
      "|Jen          |Finance   |3900  |3000|\n",
      "|Kumar        |Marketing |2000  |null|\n",
      "|Jeff         |Marketing |3000  |null|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "df.withColumn('lag', lag('salary',1).over(windowSpec)).show(truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|lag |\n",
      "+-------------+----------+------+----+\n",
      "|James        |Sales     |3000  |null|\n",
      "|James        |Sales     |3000  |3000|\n",
      "|Robert       |Sales     |4100  |3000|\n",
      "|Saif         |Sales     |4100  |4100|\n",
      "|Michael      |Sales     |4600  |4100|\n",
      "|Maria        |Finance   |3000  |null|\n",
      "|Scott        |Finance   |3300  |3000|\n",
      "|Jen          |Finance   |3900  |3300|\n",
      "|Kumar        |Marketing |2000  |null|\n",
      "|Jeff         |Marketing |3000  |2000|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### lead Window Function ü•∫\n",
    "* Returns values from next row "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "df.withColumn('Lead',lead('salary', 2).over(windowSpec)).show(truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|Lead|\n",
      "+-------------+----------+------+----+\n",
      "|James        |Sales     |3000  |4100|\n",
      "|James        |Sales     |3000  |4100|\n",
      "|Robert       |Sales     |4100  |4600|\n",
      "|Saif         |Sales     |4100  |null|\n",
      "|Michael      |Sales     |4600  |null|\n",
      "|Maria        |Finance   |3000  |3900|\n",
      "|Scott        |Finance   |3300  |null|\n",
      "|Jen          |Finance   |3900  |null|\n",
      "|Kumar        |Marketing |2000  |null|\n",
      "|Jeff         |Marketing |3000  |null|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "df.withColumn('Lead',lead('salary', 1).over(windowSpec)).show(truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|Lead|\n",
      "+-------------+----------+------+----+\n",
      "|James        |Sales     |3000  |3000|\n",
      "|James        |Sales     |3000  |4100|\n",
      "|Robert       |Sales     |4100  |4100|\n",
      "|Saif         |Sales     |4100  |4600|\n",
      "|Michael      |Sales     |4600  |null|\n",
      "|Maria        |Finance   |3000  |3300|\n",
      "|Scott        |Finance   |3300  |3900|\n",
      "|Jen          |Finance   |3900  |null|\n",
      "|Kumar        |Marketing |2000  |3000|\n",
      "|Jeff         |Marketing |3000  |null|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "df.withColumn('Lead',lead('salary', -1).over(windowSpec)).show(truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|Lead|\n",
      "+-------------+----------+------+----+\n",
      "|James        |Sales     |3000  |null|\n",
      "|James        |Sales     |3000  |3000|\n",
      "|Robert       |Sales     |4100  |3000|\n",
      "|Saif         |Sales     |4100  |4100|\n",
      "|Michael      |Sales     |4600  |4100|\n",
      "|Maria        |Finance   |3000  |null|\n",
      "|Scott        |Finance   |3300  |3000|\n",
      "|Jen          |Finance   |3900  |3300|\n",
      "|Kumar        |Marketing |2000  |null|\n",
      "|Jeff         |Marketing |3000  |2000|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Window Aggregate Functions ‚òÄÔ∏è"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "windowSpecAgg  = Window.partitionBy(\"department\")\n",
    "\n",
    "df.withColumn(\"row\",row_number().over(windowSpec)) \\\n",
    "  .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .where(col(\"row\")==1).select(\"department\",\"avg\",\"sum\",\"min\",\"max\") \\\n",
    "  .show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+------+-----+----+----+\n",
      "|department|   avg|  sum| min| max|\n",
      "+----------+------+-----+----+----+\n",
      "|     Sales|3760.0|18800|3000|4600|\n",
      "|   Finance|3400.0|10200|3000|3900|\n",
      "| Marketing|2500.0| 5000|2000|3000|\n",
      "+----------+------+-----+----+----+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Date and Timestamp Functions üìÖ\n",
    "* Date and Timestamp Functions are supported on DataFrame and SQL queries and they work similarly to traditional SQL, Date and Time are very important if you are using PySpark for ETL. Most of all these functions accept input as, Date type, Timestamp type, or String. If a String used, it should be in a default format that can be cast to date.\n",
    "\n",
    "* DateType default format is yyyy-MM-dd \n",
    "* TimestampType default format is yyyy-MM-dd HH:mm:ss.SSSS\n",
    "* Returns null if the input is a string that can not be cast to Date or Timestamp."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Date Functions "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "# lets build the data \n",
    "data=[[\"1\",\"2020-02-01\"],[\"2\",\"2019-03-01\"],[\"3\",\"2021-03-01\"]]\n",
    "df=spark.createDataFrame(data,[\"id\",\"input\"])\n",
    "df.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+----------+\n",
      "| id|     input|\n",
      "+---+----------+\n",
      "|  1|2020-02-01|\n",
      "|  2|2019-03-01|\n",
      "|  3|2021-03-01|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "df.select(current_date().alias('current_date')).show(1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------------+\n",
      "|current_date|\n",
      "+------------+\n",
      "|  2021-09-24|\n",
      "+------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "# The below example uses date_format() to parses the date and converts from yyyy-dd-mm to MM-dd-yyyy format.\n",
    "df.select(\n",
    "    col('input'),\n",
    "    date_format(col('input'), 'MM-dd-yyyy').alias('date_format')\n",
    ").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+-----------+\n",
      "|     input|date_format|\n",
      "+----------+-----------+\n",
      "|2020-02-01| 02-01-2020|\n",
      "|2019-03-01| 03-01-2019|\n",
      "|2021-03-01| 03-01-2021|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "source": [
    "# Below example converts string in date format yyyy-MM-dd to a DateType yyyy-MM-dd using to_date(). You can also use this to convert into any specific format. PySpark supports all patterns supports on Java DateTimeFormatter.\n",
    "\n",
    "df.select(\n",
    "    col('input'),\n",
    "    to_date(col('input'), 'yyy-MM-dd').alias('to_date')\n",
    ").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+----------+\n",
      "|     input|   to_date|\n",
      "+----------+----------+\n",
      "|2020-02-01|2020-02-01|\n",
      "|2019-03-01|2019-03-01|\n",
      "|2021-03-01|2021-03-01|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "# The below example returns the difference between two dates using datediff().\n",
    "df.select(\n",
    "    col('input'),\n",
    "    datediff(current_date(), col('input')).alias('datediff')\n",
    ").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+--------+\n",
      "|     input|datediff|\n",
      "+----------+--------+\n",
      "|2020-02-01|     601|\n",
      "|2019-03-01|     938|\n",
      "|2021-03-01|     207|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "# The below example returns the months between two dates using months_between().\n",
    "\n",
    "df.select(\n",
    "    col('input'),\n",
    "    months_between(current_date(), col('input')).alias('months_between')\n",
    ").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+--------------+\n",
      "|     input|months_between|\n",
      "+----------+--------------+\n",
      "|2020-02-01|   19.74193548|\n",
      "|2019-03-01|   30.74193548|\n",
      "|2021-03-01|    6.74193548|\n",
      "+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "# The below example truncates the date at a specified unit using trunc().\n",
    "df.select(col(\"input\"), \n",
    "    trunc(col(\"input\"),\"Month\").alias(\"Month_Trunc\"), \n",
    "    trunc(col(\"input\"),\"Year\").alias(\"Month_Year\"), \n",
    "    trunc(col(\"input\"),\"Month\").alias(\"Month_Trunc\")\n",
    "   ).show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+-----------+----------+-----------+\n",
      "|     input|Month_Trunc|Month_Year|Month_Trunc|\n",
      "+----------+-----------+----------+-----------+\n",
      "|2020-02-01| 2020-02-01|2020-01-01| 2020-02-01|\n",
      "|2019-03-01| 2019-03-01|2019-01-01| 2019-03-01|\n",
      "|2021-03-01| 2021-03-01|2021-01-01| 2021-03-01|\n",
      "+----------+-----------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "source": [
    "# Here we are adding and subtracting date and month from a given input.\n",
    "df.select(\n",
    "    col('input'),\n",
    "    add_months(col('input'), 3).alias('add_months'),\n",
    "    add_months(col('input'), -3).alias('sub_months'),\n",
    "    date_add(col('input'), 3).alias('date_add'),\n",
    "    date_add(col('input'), -3).alias('date_sub')\n",
    ").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+----------+----------+----------+----------+\n",
      "|     input|add_months|sub_months|  date_add|  date_sub|\n",
      "+----------+----------+----------+----------+----------+\n",
      "|2020-02-01|2020-05-01|2019-11-01|2020-02-04|2020-01-29|\n",
      "|2019-03-01|2019-06-01|2018-12-01|2019-03-04|2019-02-26|\n",
      "|2021-03-01|2021-06-01|2020-12-01|2021-03-04|2021-02-26|\n",
      "+----------+----------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "df.select(col(\"input\"), \n",
    "     year(col(\"input\")).alias(\"year\"), \n",
    "     month(col(\"input\")).alias(\"month\"), \n",
    "     next_day(col(\"input\"),\"Sunday\").alias(\"next_day\"), \n",
    "     weekofyear(col(\"input\")).alias(\"weekofyear\") \n",
    "  ).show()\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+----+-----+----------+----------+\n",
      "|     input|year|month|  next_day|weekofyear|\n",
      "+----------+----+-----+----------+----------+\n",
      "|2020-02-01|2020|    2|2020-02-02|         5|\n",
      "|2019-03-01|2019|    3|2019-03-03|         9|\n",
      "|2021-03-01|2021|    3|2021-03-07|         9|\n",
      "+----------+----+-----+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "df.select(col(\"input\"),  \n",
    "     dayofweek(col(\"input\")).alias(\"dayofweek\"), \n",
    "     dayofmonth(col(\"input\")).alias(\"dayofmonth\"), \n",
    "     dayofyear(col(\"input\")).alias(\"dayofyear\"), \n",
    "  ).show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+---------+----------+---------+\n",
      "|     input|dayofweek|dayofmonth|dayofyear|\n",
      "+----------+---------+----------+---------+\n",
      "|2020-02-01|        7|         1|       32|\n",
      "|2019-03-01|        6|         1|       60|\n",
      "|2021-03-01|        2|         1|       60|\n",
      "+----------+---------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Timestamp Functions ‚åõÔ∏è‚åõÔ∏è"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "data=[[\"1\",\"02-01-2020 11 01 19 06\"],[\"2\",\"03-01-2019 12 01 19 406\"],[\"3\",\"03-01-2021 12 01 19 406\"]]\n",
    "df2=spark.createDataFrame(data,[\"id\",\"input\"])\n",
    "df2.show(truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+-----------------------+\n",
      "|id |input                  |\n",
      "+---+-----------------------+\n",
      "|1  |02-01-2020 11 01 19 06 |\n",
      "|2  |03-01-2019 12 01 19 406|\n",
      "|3  |03-01-2021 12 01 19 406|\n",
      "+---+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "source": [
    "df2.select(current_timestamp().alias('Current_timestamp')).show(truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------------+\n",
      "|Current_timestamp         |\n",
      "+--------------------------+\n",
      "|2021-09-24 12:31:34.285474|\n",
      "|2021-09-24 12:31:34.285474|\n",
      "|2021-09-24 12:31:34.285474|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "source": [
    "import pyspark.sql.functions as sf"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "source": [
    "df2.select(\n",
    "    col('input'),\n",
    "    sf.to_timestamp(col('input'), 'MM-dd-yyyy HH mm ss SSS').alias('to_timestamp')\n",
    ").show(truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------------------+-----------------------+\n",
      "|input                  |to_timestamp           |\n",
      "+-----------------------+-----------------------+\n",
      "|02-01-2020 11 01 19 06 |2020-02-01 11:01:19.06 |\n",
      "|03-01-2019 12 01 19 406|2019-03-01 12:01:19.406|\n",
      "|03-01-2021 12 01 19 406|2021-03-01 12:01:19.406|\n",
      "+-----------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "source": [
    "data=[[\"1\",\"2020-02-01 11:01:19.06\"],[\"2\",\"2019-03-01 12:01:19.406\"],[\"3\",\"2021-03-01 12:01:19.406\"]]\n",
    "df3=spark.createDataFrame(data,[\"id\",\"input\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "source": [
    "df3.show(truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+-----------------------+\n",
      "|id |input                  |\n",
      "+---+-----------------------+\n",
      "|1  |2020-02-01 11:01:19.06 |\n",
      "|2  |2019-03-01 12:01:19.406|\n",
      "|3  |2021-03-01 12:01:19.406|\n",
      "+---+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "source": [
    "df3.select(\n",
    "    col('input'),\n",
    "    sf.hour(col('input')).alias('hour'),\n",
    "    sf.minute(col('input')).alias('minute'),\n",
    "    sf.second(col('input')).alias('second')\n",
    ").show(truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------------------+----+------+------+\n",
      "|input                  |hour|minute|second|\n",
      "+-----------------------+----+------+------+\n",
      "|2020-02-01 11:01:19.06 |11  |1     |19    |\n",
      "|2019-03-01 12:01:19.406|12  |1     |19    |\n",
      "|2021-03-01 12:01:19.406|12  |1     |19    |\n",
      "+-----------------------+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DataFrame with Examples üìä\n",
    "* You can manually create a PySpark DataFrame using toDF() and createDataFrame() methods, both these function takes different signatures in order to create DataFrame from existing RDD, list, and DataFrame.\n",
    "* You can also create PySpark DataFrame from data sources like TXT, CSV, JSON, ORV, Avro, Parquet, XML formats by reading from HDFS, S3, DBFS, Azure Blob file systems e.t.c.\n",
    "* Finally, PySpark DataFrame also can be created by reading data from RDBMS Databases and NoSQL databases.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "source": [
    "# lets create a dataset\n",
    "columns = ['langugage','users_count']\n",
    "data = [('Java', '20000'), ('Python','100000'),('Scala', '3000')]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "source": [
    "# create the dataframe\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "rdd.collect()\n",
    "rdd"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[318] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "metadata": {},
     "execution_count": 96
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "source": [
    "# using to_DF()\n",
    "dfFromRDD1 = rdd.toDF()\n",
    "dfFromRDD1"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[_1: string, _2: string]"
      ]
     },
     "metadata": {},
     "execution_count": 97
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "source": [
    "dfFromRDD1.printSchema()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "source": [
    "dfFromRDD1 = rdd.toDF(columns)\n",
    "dfFromRDD1.printSchema()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n",
      " |-- langugage: string (nullable = true)\n",
      " |-- users_count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using `createDataFrame()`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "source": [
    "dfFromRDD2 = spark.createDataFrame(rdd).toDF(*columns)\n",
    "dfFromRDD2"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[langugage: string, users_count: string]"
      ]
     },
     "metadata": {},
     "execution_count": 100
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "source": [
    "dfFromRDD2.printSchema()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n",
      " |-- langugage: string (nullable = true)\n",
      " |-- users_count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "source": [
    "dfFromData2 = spark.createDataFrame(data).toDF(*columns)\n",
    "dfFromData2.printSchema()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n",
      " |-- langugage: string (nullable = true)\n",
      " |-- users_count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "source": [
    "dfFromData2.collect()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Row(langugage='Java', users_count='20000'),\n",
       " Row(langugage='Python', users_count='100000'),\n",
       " Row(langugage='Scala', users_count='3000')]"
      ]
     },
     "metadata": {},
     "execution_count": 104
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using `createDataFrame()` with row type ü§®ü§®"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "source": [
    "import pyspark.sql as sql\n",
    "rowData = map(lambda x : sql.Row(*x), data)\n",
    "dfFromData3 = spark.createDataFrame(rowData, columns)\n",
    "dfFromData3.printSchema()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n",
      " |-- langugage: string (nullable = true)\n",
      " |-- users_count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "source": [
    "dfFromData3.collect()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Row(langugage='Java', users_count='20000'),\n",
       " Row(langugage='Python', users_count='100000'),\n",
       " Row(langugage='Scala', users_count='3000')]"
      ]
     },
     "metadata": {},
     "execution_count": 108
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create DataFrame with schema üí™üí™\n",
    "* If you wanted to specify the column names along with their data types, you should create the StructType schema first and then assign this while creating a DataFrame."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "data2 = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n",
    "  ]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "source": [
    "schema = StructType([\n",
    "    StructField('firstname', StringType(), True), \\\n",
    "    StructField(\"middlename\",StringType(),True), \\\n",
    "    StructField(\"lastname\",StringType(),True), \\\n",
    "    StructField(\"id\", StringType(), True), \\\n",
    "    StructField(\"gender\", StringType(), True), \\\n",
    "    StructField(\"salary\", IntegerType(), True) \\\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "source": [
    "df = spark.createDataFrame(data=data2, schema=schema)\n",
    "df.printSchema()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "source": [
    "df.show(truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|id   |gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|James    |          |Smith   |36636|M     |3000  |\n",
      "|Michael  |Rose      |        |40288|M     |4000  |\n",
      "|Robert   |          |Williams|42114|M     |4000  |\n",
      "|Maria    |Anne      |Jones   |39192|F     |4000  |\n",
      "|Jen      |Mary      |Brown   |     |F     |-1    |\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create DataFrame using data sources üîîüîîüîî\n",
    "* `df2 = spark.read.csv(\"/src/resources/file.csv\")`\n",
    "* `df2 = spark.read.text(\"/src/resources/file.txt\")`\n",
    "* `df2 = spark.read.json(\"/src/resources/file.json\")`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Convert the DataFrame to Pandas DF üêºüêº"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "source": [
    "pandasDF = df.toPandas()\n",
    "pandasDF"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>firstname</th>\n",
       "      <th>middlename</th>\n",
       "      <th>lastname</th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>James</td>\n",
       "      <td></td>\n",
       "      <td>Smith</td>\n",
       "      <td>36636</td>\n",
       "      <td>M</td>\n",
       "      <td>3000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Michael</td>\n",
       "      <td>Rose</td>\n",
       "      <td></td>\n",
       "      <td>40288</td>\n",
       "      <td>M</td>\n",
       "      <td>4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Robert</td>\n",
       "      <td></td>\n",
       "      <td>Williams</td>\n",
       "      <td>42114</td>\n",
       "      <td>M</td>\n",
       "      <td>4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Maria</td>\n",
       "      <td>Anne</td>\n",
       "      <td>Jones</td>\n",
       "      <td>39192</td>\n",
       "      <td>F</td>\n",
       "      <td>4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jen</td>\n",
       "      <td>Mary</td>\n",
       "      <td>Brown</td>\n",
       "      <td></td>\n",
       "      <td>F</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  firstname middlename  lastname     id gender  salary\n",
       "0     James                Smith  36636      M    3000\n",
       "1   Michael       Rose            40288      M    4000\n",
       "2    Robert             Williams  42114      M    4000\n",
       "3     Maria       Anne     Jones  39192      F    4000\n",
       "4       Jen       Mary     Brown             F      -1"
      ]
     },
     "metadata": {},
     "execution_count": 115
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## User Defined Functions üé®üé®\n",
    "* UDF‚Äôs are the most expensive operations hence use them only you have no choice and when essential. In the later section of the article, I will explain why using UDF‚Äôs is an expensive operation in detail.\n",
    "* PySpark UDF‚Äôs are similar to UDF on traditional databases. In PySpark, you create a function in a Python syntax and wrap it with PySpark SQL udf() or register it as udf and use it on DataFrame and SQL respectively.\n",
    "* UDF‚Äôs are used to extend the functions of the framework and re-use these functions on multiple DataFrame‚Äôs. For example, you wanted to convert every first letter of a word in a name string to a capital case; PySpark build-in features don‚Äôt have this function hence you can create it a UDF and reuse this as needed on many Data Frames. UDF‚Äôs are once created they can be re-used on several DataFrame‚Äôs and SQL expressions.\n",
    "* Before you create any UDF, do your research to check if the similar function you wanted is already available in Spark SQL Functions. PySpark SQL provides several predefined common functions and many more new functions are added with every release. hence, It is best to check before you reinventing the wheel."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Steps in creating and using UDF ü™úü™ú\n",
    "* Create a Python Function\n",
    "* Convert a Python function to PySpark UDF\n",
    "* Using UDF with DataFrame\n",
    "    * Using UDF with PySpark DataFrame withColumn()\n",
    "    * Registering PySpark UDF & use it on SQL"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "source": [
    "columns = [\"Seqno\",\"Name\"]\n",
    "data = [(\"1\", \"john jones\"),\n",
    "    (\"2\", \"tracey smith\"),\n",
    "    (\"3\", \"amy sanders\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "\n",
    "df.show(truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+------------+\n",
      "|Seqno|Name        |\n",
      "+-----+------------+\n",
      "|1    |john jones  |\n",
      "|2    |tracey smith|\n",
      "|3    |amy sanders |\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "source": [
    "def convertCase(str):\n",
    "    '''\n",
    "    Returns the input string with all words first letter capitalized.\n",
    "    '''\n",
    "    resStr = ''\n",
    "    arr = str.split(' ')\n",
    "    for x in arr:\n",
    "        resStr = resStr + x[0:1].upper() + x[1:len(x)] + ' '\n",
    "    return resStr"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "source": [
    "convertUDF = udf(lambda z : convertCase(z), StringType())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "source": [
    "df.select(\n",
    "    col('Seqno'),\n",
    "    convertUDF(col('Name')).alias('Name')\n",
    ").show(truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+-------------+\n",
      "|Seqno|Name         |\n",
      "+-----+-------------+\n",
      "|1    |John Jones   |\n",
      "|2    |Tracey Smith |\n",
      "|3    |Amy Sanders  |\n",
      "+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "lst = ['  abcd  ',  'abcd    aa bb ccc   ']\n",
    "for i in lst:\n",
    "    i = ' '.join((i.strip()).split())\n",
    "    print(i)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "abcd\n",
      "abcd aa bb ccc\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "34377170317761be4b2ba4a4ec8dbf214d9d012bf2f308360cf1daccf5fb40ce"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}